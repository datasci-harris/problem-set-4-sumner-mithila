---
title: "PSet 4"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sumner Perera (CNet ID: 12403312)
    - Partner 2 (name and cnet ID): Mithila Iyer (CNet ID: 12414493)
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\S\P\*\* \*\*\M\I\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. (SP) Late coins used this pset: \*\*\1\*\* Late coins left after submission: \*\*\1\*\*
    (MI): Late coins used this pset: \*\*\1\*\* Late coins left after submission: \*\*\0\*\* 
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. After reviewing the entirety of the pset and the questions that are asked, I pulled the following variables: 

- PRVDR_CTGRY_CD: Provider type 
- PRVDR_CTGRY_SBTYP_CD: Subtype code 
- PRVDR_NUM: CMS Certification Number
- PGM_TRMNTN_CD: Provider termination code 
- ZIP_CD: Zip code 
- FAC_NAME: Provider name 
- TRMNTN_EXPRTN_DT: Date the provider was terminated 

2. 
```{python}
# Import the file from my local drive 
import pandas as pd 
import os 
base_path = r"C:\Users\12019\OneDrive - The University of Chicago\Year 2\Python II\PS4"
path = os.path.join(base_path, "pos2016.csv")
pos2016 = pd.read_csv(path)

# Subset to PRVDR_CTGRY_CD = 01 and PRVDR_CTGRY_SBTYP_CD = 01 
subset_16 = pos2016[(pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2016["PRVDR_CTGRY_CD"] == 1)]

# Count the number of rows 
total_16 = len(subset_16)
total_16
```
Subsetting to short-term hospitals gives a dataset with 7245 total facilities. According to the linked paper, there are nearly 5,000 short-term acute care hospitals which is a smaller subset than what I found with subsetting. Another data source (https://www.aha.org/statistics/fast-facts-us-hospitals) cites a total of 6,120 hospitals in the US. Both sources cite a lower number than what we found which suggests that this number may be inaccurate. It could differ because there are hospitals listed which have actually closed or not operational. 
    
3.
```{python}
#Load in datasets 
path1 = os.path.join(base_path, "pos2017.csv")
pos2017 = pd.read_csv(path1)

path2 = os.path.join(base_path, "pos2018.csv")
pos2018 = pd.read_csv(path2, encoding='latin1')

#Note: Had to do the encoding='latin1' addition to account for errors which said: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x98 in position 11970: invalid start byte

path3 = os.path.join(base_path, "pos2019.csv")
pos2019 = pd.read_csv(path3, encoding='latin1')

# Drawn from ChatGPT. Asked "How do you append multiple data sets with the same columns?"
pos2016["data_year"] = 2016
pos2017["data_year"] = 2017
pos2018["data_year"] = 2018
pos2019["data_year"] = 2019
combined = pd.concat([pos2016, pos2017, pos2018, pos2019], axis=0)

# Plot the number of observations by year  
import altair as alt
alt.data_transformers.disable_max_rows()
alt.renderers.enable('png') 

alt.Chart(combined).mark_bar().encode(
   x=("data_year:O"), 
   y="count()"
)
```

4. 
```{python}
#Create a dataframe for unique values. Drawn from ChatGPT, asked "How to group by unique values in a column and then plot all unique observations by year in altair?"
unique = combined.groupby("data_year")["PRVDR_NUM"].nunique().reset_index()
unique = unique.rename(columns={"PRVDR_NUM": "unique_count"})

#Plot the unique hospitals
alt.Chart(unique).mark_bar().encode(
   x=alt.X("data_year:O", title="Year"), 
   y=alt.Y("unique_count:Q", title="Unique Hospitals")
)
```
This bar chart is identical to the previous bar chart which suggests that there are no duplicate listing of hospitals in the data based on individual CMS certification numbers, and all the data has been cleaned. 


## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#Drawing in combined file and checking it 
import pandas as pd
import os 
path = "/Users/mithilaiyer/Documents/GitHub/psetdata/combined_part1.csv"
combined_hosp = pd.read_csv(path4, low_memory=False) 
combined_hosp.head()

#Finding hospitals that are active in 2016 (where PGM_TRMNTN_CD is zero) 
active_in_2016 = combined_hosp[(combined_hosp['data_year'] == 2016) & (combined_hosp['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM'].unique()

#Finding hospitals that are closed (according to termination code)
hosp_closed = combined_hosp[(combined_hosp['PRVDR_NUM'].isin(active_in_2016)) & (combined_hosp['data_year'].isin([2017, 2018, 2019])) & (combined_hosp['PGM_TRMNTN_CD']!=0)]

#Now we need to find the suspected year of closure. 
year_closure = hosp_closed.groupby('PRVDR_NUM')['data_year'].min().reset_index()
year_closure.columns = ['PRVDR_NUM', 'YEAR_CLOSURE']
sus_closed_hosp = combined_hosp[combined_hosp['PRVDR_NUM'].isin(year_closure['PRVDR_NUM'])]

#Now we need to check for duplicates and do a tiny bit of cleaning (mergeing and keeping the reqd columns)
sus_closed_hosp = (
    sus_closed_hosp
    .drop_duplicates(subset=['PRVDR_NUM'])
    .merge(year_closure, on="PRVDR_NUM", how="left")
    [[ 'FAC_NAME', 'ZIP_CD', 'YEAR_CLOSURE']]
)

print(len(sus_closed_hosp))
#Source: ChatGPT used for the duplicates check and the merge
```
We see 174 hospitals meet the criteria 

2. 
```{python}
#Sorting by hospital name and printing the first 10 rows
sus_closed_hosp = sus_closed_hosp.sort_values(by='FAC_NAME')
result = sus_closed_hosp[['FAC_NAME', 'YEAR_CLOSURE']].head(10)
print(result)
```

3. 
```{python}
#First, we will need to consolidate the dataset to find hospitals that were active in 2016. From this, we will be able to group by zip code to find the number of hospitals in each zip code
active_hosp = combined_hosp[combined_hosp ['PGM_TRMNTN_CD'] ==0]
#Briefly converting zip code to string, this will help our merge in 3.2
active_hosp.loc[:, 'ZIP_CD'] = active_hosp['ZIP_CD'].astype(str).str.split('.').str[0]
grouped_hosp = active_hosp.groupby(['ZIP_CD', 'data_year']).size().reset_index(name='NUM_HOSP')
grouped_hosp['Diff_Count'] = grouped_hosp.groupby('ZIP_CD')['NUM_HOSP'].diff()
#Converting zip code back to numeric for checking in the function
grouped_hosp['ZIP_CD'] = grouped_hosp['ZIP_CD'].astype(int)

#Next we can create a function that will gauge increases and decreases
def checker(row):
    zip = row['ZIP_CD']
    closure_year = row['YEAR_CLOSURE']
    prev_year = grouped_hosp[(grouped_hosp['ZIP_CD']== zip) & (grouped_hosp['data_year']== closure_year)]
    after_year = grouped_hosp[(grouped_hosp['ZIP_CD']== zip) & (grouped_hosp['data_year']== closure_year + 1)]

    if not prev_year.empty and not after_year.empty:
        return after_year['NUM_HOSP'].values[0] < prev_year['NUM_HOSP'].values[0]
    return True

#Now filtering the increases and decreases through the function
sus_closed_hosp_filt = sus_closed_hosp[sus_closed_hosp.apply(checker, axis=1)]
#Source: ChatGPT used for help thinking about how to gauge increases/decreases  and for applying it to each row of the suspected closed hospitals.
```


    a. 
```{python}
closed_hosp_names_filt = sus_closed_hosp_filt ['FAC_NAME']
merg_acq = sus_closed_hosp[~sus_closed_hosp['FAC_NAME'].isin(closed_hosp_names_filt)]
print(len(merg_acq))
```
27 hospitals are suspected to be mergers and acquisitions

    b. 
```{python}
#In order to correct for mergers and acquisitions, we chose to delete the mergers from the old dataset
merg_acq_names = merg_acq['FAC_NAME']
sus_closed_hosp_filt = sus_closed_hosp_filt[~sus_closed_hosp_filt['FAC_NAME'].isin(merg_acq_names)]

print(len(sus_closed_hosp_filt))
```
147 hospitals remain after correcting for mergers

    c.
```{python}
#Renaming the filtered dataset for clarity 
filtered_data = sus_closed_hosp_filt
#Now we print the top 10 rows of this corrected list
filtered_data = filtered_data.sort_values(by='FAC_NAME')
result2 = filtered_data [['FAC_NAME']].head(10)
print(result2)
```


## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are: xml (this stores metadata about the shapefile and information about variables), dbf (this contains attribute information in a table), prj (this contains projection and coordinate system information), shp (this is the main file that has the feature geometry), and shx(this is the index file of the feature geometry).
    b. The datasets have the following sizes: xml (16kb), dbf (6.4mb), prj (4kb), shp (837.5mb), shx (266kb)
2. 

```{python}
#Loading in the shapefile and geodata modules
import geopandas as gpd 
import pandas as pd
import matplotlib.pyplot as plt
filepath = "/Users/mithilaiyer/Documents/GitHub/psetdata/shp/gz_2010_us_860_00_500k.shp" 
pset4_geo = gpd.read_file(filepath)
#Restricted to Texas zipcodes
subset_tx = pset4_geo[pset4_geo['ZCTA5'].str[:3].between('750', '799')]
#According to the question, we should be using all hospitals in 2016 in Texas. This is how we will filter - though first we should convert the zip code var to string. 

combined_hosp['ZIP_CD'] = combined_hosp['ZIP_CD'].astype(str).str.split('.').str[0]
tx_hosp = combined_hosp[
    (combined_hosp['ZIP_CD'].astype(str).str[:3].between('750', '799')) & 
    (combined_hosp['data_year'] == 2016)
]
#Grouping by zip code to get the number of hospitals
tx_hosp_group = tx_hosp.groupby('ZIP_CD').size().reset_index(name='Num_Hosp')
#Pre-merge cleaning. As we are merging on zip code, we will need to convert back to int and rename the hosp data zip code variable to match the spatial data
tx_hosp_group['ZIP_CD'] = tx_hosp_group['ZIP_CD'].astype(int)
tx_hosp_group.rename(columns={'ZIP_CD': 'ZCTA5'}, inplace=True)
subset_tx['ZCTA5'] = subset_tx['ZCTA5'].astype(int)
#Merging and fixing for zip codes without hospitals (by filling with 0)
merged_tex = subset_tx.merge(tx_hosp_group, on='ZCTA5', how='left')
merged_tex['Num_Hosp'].fillna(0, inplace=True)

#Chloropleth
merged_tex.plot(column='Num_Hosp', cmap='coolwarm', 
linewidth=0.3, edgecolor='0.8', legend=True)
plt.title('Texas Hospitals per ZIP in 2016')
plt.axis('off')
plt.show()
#Source: ChatGPT for chloropleth refinement
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
#Calculating centroids
zips_all_centroids = pset4_geo.copy()
zips_all_centroids['centroid'] = zips_all_centroids.centroid
zips_all_centroids.set_geometry('centroid')

#Information on dimensions and columns
dimensions = zips_all_centroids.shape
print("Dimensions:", dimensions)
print(zips_all_centroids.info())
#Source: ChatGPT used for understanding how to print dimension information.
```
The dataframe has 33,120 observations or unique zip codes. It has 7 columns (4 object, 1 integer, and 2 geometry). The centroid column is calculated from the geometry column and is the mean of the points in the polygon. GeoID is the unique geographical identifier, ZCTAS and NAME describe the zip code, and LSAD is the legal statistical area definition. Finally, census area is the calculated area derived from the ungeneralized area.

2. 
```{python}
#Creating the Texas centroids GeoDataframe and sorting to check all the zip codes are in there 
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799)]
zips_texas_centroids = zips_texas_centroids.sort_values(by='ZCTA5', ascending=True)

#Creating the Texas + bordering states (New Mexico, Oklahoma, Arkansas, Louisiana) GeoDataframe and checking all codes are in there.

zips_texas_borderstates_centroids = zips_all_centroids[
    (
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(700, 714) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(716, 729) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(730, 749) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(870, 885)
    )
]
zips_texas_borderstates_centroids = zips_texas_borderstates_centroids.sort_values(by='ZCTA5', ascending=True)
#Source: ChatGPT to determine how to combine multiple subsets
```
There are 1935 unique zip codes in the Texas subset, and 4057 unique codes in the Texas and its bordering states subset. 


3. 

```{python}
#The desired subset zips_withhospitals_centroids should contain only zipcodes with at least 1 hospital in 2016, from the zip codes in the subsets with bordering states of TX. First, we isolate the hospital data for hospitals active in 2016. We've already done this in 2.1, but redefining here
active_in_2016 = combined_hosp[(combined_hosp['data_year'] == 2016) & (combined_hosp['PGM_TRMNTN_CD'] == 0)][['data_year', 'ZIP_CD', 'PRVDR_NUM']].drop_duplicates()
active_in_2016['ZIP_CD'] = active_in_2016['ZIP_CD'].astype(str).str.replace('.0', '')

#Changing zip code for border states centroid to string 
zips_texas_borderstates_centroids['ZCTA5'] = zips_texas_borderstates_centroids['ZCTA5'].astype(str).str.replace('.0', '')
#Merging with the border states centroid
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    active_in_2016[['ZIP_CD']],
    left_on='ZCTA5',
    right_on='ZIP_CD',
    how='inner'
)
```
Ultimately we want rows with an active hospital in 2016 that has a zipcode aligning with the TX and borderstates dataset to be present in the new dataset (zips_withhospital_centroids). We achieved this through a inner merge on zipcodes.


4. 
    a. 
```{python}
#In order to do the subsetted join with 10 zip codes, lets first sort by zip code then take the top 10
zips_top10_texas_centroids = zips_texas_centroids[:10]

#Now we import the timer 
import time 
start = time.time()
closest_hosp = gpd.sjoin_nearest(
    zips_top10_texas_centroids,
    zips_withhospital_centroids,
    how='inner', distance_col="Dist"
)

end = time.time()
elapsed_time = (end - start)


print(f'{elapsed_time:.3f}')
full_merge_mins = (elapsed_time * (zips_texas_centroids.shape[0] / 10)) / 60
print(f'{full_merge_mins:.3f}')

#Source: ChatGPT for the time calculations and printing in the correct format 
```
It will take 0.547 seconds to complete the subsetted merge
It will take 1.765 minutes to complete the full merge

    b. 
```{python}
#Now, we do the full merge.
start = time.time ()
closest_hosp_full = gpd.sjoin_nearest(
    zips_texas_centroids,
    zips_withhospital_centroids,
    how='inner', distance_col="Dist"
)
end = time.time()
elapsed_time = (end - start)
print(f'{elapsed_time/60:.3f}')
```
It took 1.877 mins to complete the full merge

c. 
```{python}
#The PRJ file states that the unit is degree. The United States Geographical Survey (https://www.usgs.gov/faqs/how-much-distance-does-a-degree-minute-and-second-cover-your-maps) states that 1 degree is approximately 69 miles. The reason that the unit is degrees is because it is projection based, not geography based CRS. 
```


5. 
    a.
```{python}
average_distance = closest_hosp_full['Dist'].mean()
print(average_distance)
```
The average distance is 0.026840066531970706. This doesnt seem right, but we haven't adjusted for degrees yet.

    b.
```{python}
result = average_distance * 69
print(result)
```
This seems more plausible. The average distance is 1.8519645907059787, or nearly 2 miles when we multiply by the degree factor

    c.
```{python}
#Plotting the chloropleth
closest_hosp_full.plot(column='Dist', cmap='Blues',
linewidth=0.3, edgecolor='0.8', legend=True)
plt.title("Avg Dist to Hospital", fontsize=12)
plt.axis("off")
```


    
## Effects of closures on access in Texas (15 pts)

1. 
```{python}
#Using the clean data, create a subset with only TX zip codes
zip_codes_tx = filtered_data[(filtered_data['ZIP_CD'] >= 75000) & (filtered_data['ZIP_CD'] <= 79999)]

#Drawn from ChatGPT. Asked "Let's say I have a data frame with a column of values. How do I create a new table that shows me in one column all the unique values in that column and how many times they appeared?" This gives a table with all Texas zip codes and the number of closures within each zip code. 
tx_zips_closures = zip_codes_tx["ZIP_CD"].value_counts().reset_index()
tx_zips_closures

```

2. 
```{python}
#Plot the cloropleth only for the zip codes that we identified in the previous question. Drawn from ChatGPT. Asked "If I have a list of values in a dataframe that are a subset of values from a column in a geopandas shp file. How do I plot only those subset in a cloropleth?"

#First, create a list of the unique values and filter the geo shp data to only those zips in TX
import matplotlib.pyplot as plt 
subset_values = tx_zips_closures["ZIP_CD"]
subset_geo = subset_tx[subset_tx["ZCTA5"].isin(subset_values)]

#Next, plot the cloropleth
subset_geo.plot(color="brown")
plt.axis("off")
plt.show()

#Figure out the number of these regions. 
print(len(tx_zips_closures))
```
There are 28 zip codes in Texas that have at least 1 closure in 2016-2019.

3. 

```{python}
#Already created a GeoDataFrame of the directly affected zip codes. Now, create a 10 mile buffer around them. Drawn from ChatGPT, asked "How to do a spatial join between two geodata frames?" and had to figure out "UserWarning: Geometry is in a geographic CRS" error. 

#Create a geodataframe for the buffered regions 
buffer_m = 10*1609.34
tx_zips_buffered = subset_geo.copy().to_crs(epsg=32633)
tx_zips_buffered["buffered"] = tx_zips_buffered.geometry.buffer(buffer_m)

#Next, with a bit of renaming first, do the join to the geodataframe only for TX
all_tx = subset_tx.to_crs(epsg=32633)
joined = gpd.sjoin_nearest(tx_zips_buffered, all_tx, how="inner", distance_col="distance")

#How many indirectly affected zip codes are there?
print(len(joined))
```
There are 198 indirectly affected zip codes in Texas. 

4. 

```{python}
#Need to correct back to a projected CRS instead of a geographic one. Drawn from ChatGPT for guidance. Asked "I have 3 geodataframes. One that includes all zip codes and two datasets that have subsets of zip codes. I want to create a choropleth that maps these three categories in different colors. How do I do that?"
all_tx = all_tx.to_crs(epsg=3857)
joined = joined.to_crs(epsg=3857)
subset_geo = subset_geo.to_crs(epsg=3857)

#Add a category column to each 
all_tx['category'] = "Other"
joined['category'] = "Indirect"
subset_geo['category'] = "Direct"

#Combine datasets together 
zip_combination_cp = pd.concat([all_tx, joined, subset_geo], ignore_index=True)

#Create a color map
color_map = {
    "Other": "lightgrey",
    "Indirect": "blue",
    "Direct": "lightgreen"
}
zip_combination_cp["color"] = zip_combination_cp["category"].map(color_map)

#Plot the choropleth
zip_combination_cp.plot(color=zip_combination_cp["color"], legend=True)
```

## Reflecting on the exercise (10 pts) 
1. We rely on two assumptions for suspected closures: hospitals lose an active provider label or are dropped from the dataset in the subsequent year and hospitals that "close" might actually be mergers/acquisitions. Some issues with this approach include whether there is proper labeling of active versus terminated hospitals and also lack of visibility into whether that closure is truly reflected over the year that the observation goes missing. Potential ways to fix this would be to search whether the hospital has an online record during that year to show that it is in fact active or not active. For example, if a hospital goes missing in 2017 towards the end of the year but was operational for the majority of that year -- it could still be considered active for 2017. This would be more a matter of reporting timing across quarters and an online verification could provide greater clarity on status throughout the year. 

2. We have two major assumptions when it comes to zip codes that have been affected by closures: we consider "direct" disturbance to be if that zip code has at least 1 hospital closure and another is that we consider "indirect" disturbance to be zip codes within 10 miles. This may or may not reflect true changes in zip code access to hospitals because it could be possible that some major/large hospitals serve multiple zip codes and therefore a closure of a smaller hospital may not be consequential to that zip code's access to services. Some ways to improve this would be to add a data metric of whether the merger/acquisition hospitals serve more patients and zip codes which would require outside research. 

