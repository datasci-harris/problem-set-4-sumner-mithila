---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sumner Perera (CNet ID: 12403312)
    - Partner 2 (name and cnet ID): Mithila Iyer (CNet ID: 12414493)
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\S\P\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

Provider type code 01 (PRVDR_CTGRY_CD) and subtype code 01 (PRVDR_CTGRY_SBTYP_CD)
CMS Certification Number: PRVDR_NUM
Closed if its Termination Code in the POS file lists them as an “Active Provider” in 2016
- PGM_TRMNTN_CD and the value for "Active Provider" is 00
Zip code: ZIP_CD
Name of provider: FAC_NAME 
Year of termination: TRMNTN_EXPRTN_DT

Link to 2016 filtered view: https://data.cms.gov/provider-characteristics/hospitals-and-other-facilities/provider-of-services-file-hospital-non-hospital-facilities/data/q4-2016?query=%7B%22filters%22%3A%7B%22rootConjunction%22%3A%7B%22label%22%3A%22And%22%2C%22value%22%3A%22AND%22%7D%2C%22list%22%3A%5B%5D%7D%2C%22keywords%22%3A%22%22%2C%22offset%22%3A0%2C%22limit%22%3A10%2C%22sort%22%3A%7B%22sortBy%22%3Anull%2C%22sortOrder%22%3Anull%7D%2C%22columns%22%3A%5B%22PRVDR_CTGRY_SBTYP_CD%22%2C%22PRVDR_CTGRY_CD%22%2C%22FAC_NAME%22%2C%22PRVDR_NUM%22%2C%22PGM_TRMNTN_CD%22%2C%22ZIP_CD%22%2C%22TRMNTN_EXPRTN_DT%22%5D%7D


1. After reviewing the entirety of the pset and the questions that are asked, I pulled the following variables: 

- PRVDR_CTGRY_CD: Provider type 
- PRVDR_CTGRY_SBTYP_CD: Subtype code 
- PRVDR_NUM: CMS Certification Number
- PGM_TRMNTN_CD: Provider termination code 
- ZIP_CD: Zip code 
- FAC_NAME: Provider name 
- TRMNTN_EXPRTN_DT: Date the provider was terminated

2. 
```{python}
# Import the file from my local drive 
import pandas as pd 
import os 
base_path = r"C:\Users\12019\OneDrive - The University of Chicago\Year 2\Python II\PS4"
path = os.path.join(base_path, "pos2016.csv")
pos2016 = pd.read_csv(path)

# Subset to PRVDR_CTGRY_CD = 01 and PRVDR_CTGRY_SBTYP_CD = 01 
subset_16 = pos2016[(pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2016["PRVDR_CTGRY_CD"] == 1)]

#Add year 
subset_16["data_year"] = 2016

# Count the number of rows 
total_16 = len(subset_16)
total_16
```

Subsetting to short-term hospitals gives a dataset with 7245 total facilities. According to the linked paper, there are nearly 5,000 short-term acute care hospitals which is a smaller subset than what I found with subsetting. Another data source (https://www.aha.org/statistics/fast-facts-us-hospitals) cites a total of 6,120 hospitals in the US. Both sources cite a lower number than what we found which suggests that this number may be inaccurate. It could differ because there are hospitals listed which have actually closed or not operational. 
    
3.
```{python}
# Load in other datasets 
path1 = os.path.join(base_path, "pos2017.csv")
pos2017 = pd.read_csv(path1, encoding="latin1")

path2 = os.path.join(base_path, "pos2018.csv")
pos2018 = pd.read_csv(path2, encoding="latin1")

# Note: Had to do the encoding='latin1' addition to account for errors which said: 
# UnicodeDecodeError: 'utf-8' codec can't decode byte 0x98 in position 11970: invalid start byte

path3 = os.path.join(base_path, "pos2019.csv")
pos2019 = pd.read_csv(path3, encoding="latin1")

# Subset the other datasets 
subset_17 = pos2017[(pos2017["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2017["PRVDR_CTGRY_CD"] == 1)]
subset_18 = pos2018[(pos2018["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2018["PRVDR_CTGRY_CD"] == 1)]
subset_19 = pos2019[(pos2019["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2019["PRVDR_CTGRY_CD"] == 1)]

#Add the years to each dataset before merging 
subset_17["data_year"] = 2017
subset_18["data_year"] = 2018
subset_19["data_year"] = 2019

# Drawn from ChatGPT. Asked "How to merge datasets that have the same columns but not in the same order?"
combined = pd.concat([subset_16, subset_17, subset_18, subset_19], axis=0, ignore_index=True)

# Plot the number of observations by year. 
import altair as alt
alt.renderers.enable("png")

alt.Chart(combined).transform_aggregate(
    count="count()", 
    groupby=["data_year"]
).mark_bar().encode(
  alt.X('data_year:O'), 
  alt.Y('count:Q')
)

```

4. 

```{python}
#Plot the number of unique hospitals by year. Used https://vega.github.io/vega-lite/docs/aggregate.html to get the transform aggregate function for count of distinct values. 

alt.Chart(combined).transform_aggregate(
    unique="distinct(PRVDR_NUM)", 
    groupby=["data_year"]
).mark_bar().encode(
    alt.X("data_year:O"), 
    alt.Y("unique:Q")
)
```

This plot shows that every value is unique since the count of both graphs per year are the same. 

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are: xml (this stores metadata about the shapefile and information about variables), dbf (this contains attribute information in a table), prj (this contains projection and coordinate system information), shp (this is the main file that has the feature geometry), and shx(this is the index file of the feature geometry).
    b. The datasets have the following sizes: xml (16kb), dbf (6.4mb), prj (4kb), shp (837.5mb), shx (266kb)
2. 

```{python}
#Loading in data.
import geopandas as gpd 
import pandas as pd
filepath = "/Users/mithilaiyer/Documents/GitHub/psetdata/shp/gz_2010_us_860_00_500k.shp" 
pset4_geo = gpd.read_file(filepath)
pset4_geo['ZCTA5'] = pd.to_numeric(pset4_geo['ZCTA5'], errors='coerce')
#Restricted to Texas zipcodes
subset_geo = pset4_geo[(pset4_geo['ZCTA5'] >= 750) & (pset4_geo['ZCTA5'] <= 799)]

```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
#Calculate centroids
zips_all_centroids = pset4_geo.copy()
zips_all_centroids['centroid'] = zips_all_centroids.centroid
zips_all_centroids.set_geometry('centroid')

#Information on dimensions and columns
dimensions = zips_all_centroids.shape
print("Dimensions:", dimensions)
print(zips_all_centroids.info())
#The dataframe has 33,120 observations or unique zip codes. It has 7 columns (4 object, 1 integer, and 2 geometry). The centroid column is calculated from the geometry columm and is the mean of the points in the polygon. GeoID is the unique geographical identifier, ZCTAS and NAME describe the zip code, and LSAD is the legal statistical area definition. Finally, census area is the calculated area derived from the ungeneralized area.
```

2. 
```{python}
#Creating the Texas centroids GeoDataframe and sorting to check all the zip codes are in there 
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799)]
zips_texas_centroids = zips_texas_centroids.sort_values(by='ZCTA5', ascending=True)

#Creating the Texas + bordering states (New Mexico, Oklahoma, Arkansas, Louisiana) GeoDataframe and checking all codes are in there.

zips_texas_borderstates_centroids = zips_all_centroids[
    (
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(700, 714) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(716, 729) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(730, 749) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(870, 885)
    )
]
zips_texas_borderstates_centroids = zips_texas_borderstates_centroids.sort_values(by='ZCTA5', ascending=True)

#There are 1935 unique zip codes in the Texas subset, and 4057 unique codes in the Texas and its bordering states subset. 
#Source: ChatGPT to determine how to combine multiple subsets
```

2. 
```{python}
#CRM is a way of converting something flat into spherical. Prj is projection, will tell you . it's in degres. 

```


3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
