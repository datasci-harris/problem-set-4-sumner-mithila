---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sumner Perera (CNet ID: 12403312)
    - Partner 2 (name and cnet ID): Mithila Iyer (CNet ID: 12414493)
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\S\P\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

Provider type code 01 (PRVDR_CTGRY_CD) and subtype code 01 (PRVDR_CTGRY_SBTYP_CD)
CMS Certification Number: PRVDR_NUM
Closed if its Termination Code in the POS file lists them as an “Active Provider” in 2016
- PGM_TRMNTN_CD and the value for "Active Provider" is 00
Zip code: ZIP_CD
Name of provider: FAC_NAME 
Year of suspected closure: FY_END_MO_DAY_CD 

Link to filtered view: https://data.cms.gov/provider-characteristics/hospitals-and-other-facilities/provider-of-services-file-hospital-non-hospital-facilities/data/q4-2016?query=%7B%22filters%22%3A%7B%22rootConjunction%22%3A%7B%22label%22%3A%22And%22%2C%22value%22%3A%22AND%22%7D%2C%22list%22%3A%5B%5D%7D%2C%22keywords%22%3A%22%22%2C%22offset%22%3A0%2C%22limit%22%3A10%2C%22sort%22%3A%7B%22sortBy%22%3Anull%2C%22sortOrder%22%3Anull%7D%2C%22columns%22%3A%5B%22PRVDR_CTGRY_SBTYP_CD%22%2C%22PRVDR_CTGRY_CD%22%2C%22FAC_NAME%22%2C%22PRVDR_NUM%22%2C%22PGM_TRMNTN_CD%22%2C%22ZIP_CD%22%2C%22FY_END_MO_DAY_CD%22%5D%7D 


1. After reviewing the entirety of the pset and the questions that are asked, I pulled the following variables: 

- PRVDR_CTGRY_CD: Provider type 
- PRVDR_CTGRY_SBTYP_CD: Subtype code 
- PRVDR_NUM: CMS Certification Number
- PGM_TRMNTN_CD: Provider termination code 
- ZIP_CD: Zip code 
- FAC_NAME: Provider name 
- FY_END_MO_DAY_CD: Year of suspected closure 

2. 
```{python}
# Import the file from my local drive 
import pandas as pd 
import os 
base_path = r"C:\Users\12019\\Downloads\pos2016"
path = os.path.join(base_path, "pos2016.csv")
pos2016 = pd.read_csv(path)

# Subset to PRVDR_CTGRY_CD = 01 and PRVDR_CTGRY_SBTYP_CD = 01 
subset_16 = pos2016[(pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2016["PRVDR_CTGRY_CD"] == 1)]

# Count the number of rows 
total_16 = len(subset_16)
total_16
```

Subsetting to short-term hospitals gives a dataset with 7245 total facilities. According to the linked paper, there are nearly 5,000 short-term acute care hospitals which is a smaller subset than what I found with subsetting. Another data source (https://www.aha.org/statistics/fast-facts-us-hospitals) cites a total of 6,120 hospitals in the US. Both sources cite a lower number than what we found which suggests that this number may be inaccurate. It could differ because there are hospitals listed which have actually closed or not operational. 
    
3.
```{python}
# Load in datasets 
base_path1 = r"C:\Users\12019\\Downloads\pos2017"
path1 = os.path.join(base_path1, "pos2017.csv")
pos2017 = pd.read_csv(path1)

base_path2 = r"C:\Users\12019\\Downloads\pos2018"
path2 = os.path.join(base_path2, "pos2018.csv")
pos2018 = pd.read_csv(path2, encoding='latin1')

# Note: Had to do the encoding='latin1' addition to account for errors which said: 
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x98 in position 11970: invalid start byte

base_path3 = r"C:\Users\12019\\Downloads\pos2019"
path3 = os.path.join(base_path3, "pos2019.csv")
pos2019 = pd.read_csv(path3, encoding='latin1')

# Drawn from ChatGPT. Asked "How do you append multiple data sets with the same columns?"
combined = pd.concat([pos2016, pos2017, pos2018, pos2019], axis=0)

# Plot the number of observations by year  
import altair as alt
alt.Chart(combined).mark_bar().encode(
  alt.X('year:Q'), 
  y='count()'
)

```



4. 
    a.
    b.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#Drawing in combined file and checking it 
import pandas as pd
import os 
path = "/Users/mithilaiyer/Documents/GitHub/psetdata/combined_part1.csv"
combined_hosp = pd.read_csv(path, low_memory=False) 
combined_hosp.head()

#Finding hospitals that are active in 2016 (where PGM_TRMNTN_CD is zero) 
active_in_2016 = combined_hosp[(combined_hosp['data_year'] == 2016) & (combined_hosp['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM'].unique()

#Finding hospitals that are closed (according to termination code)
hosp_closed = combined_hosp[(combined_hosp['PRVDR_NUM'].isin(active_in_2016)) & (combined_hosp['data_year'].isin([2017, 2018, 2019])) & (combined_hosp['PGM_TRMNTN_CD']!=0)]

#Now, we should check if there are any hospitals that may be in the data only in 2016. We can do this by subtracting the number of hospitals present after 2016 from the active hospitals in 2016
active_after_2016 = combined_hosp[combined_hosp['data_year'].isin([2017, 2018, 2019])]['PRVDR_NUM'].unique()
maybe_closure = [x for x in active_in_2016 if x not in active_after_2016]
print(maybe_closure)
#This prints an empty set, which means that there are no hospitals in the dataset operating solely in 2016

#Now we need to find the suspected year of closure. 
year_closure = hosp_closed.groupby('PRVDR_NUM')['data_year'].min().reset_index()
year_closure.columns = ['PRVDR_NUM', 'YEAR_CLOSURE']
sus_closed_hosp = combined_hosp[combined_hosp['PRVDR_NUM'].isin(year_closure['PRVDR_NUM'])]

#Now we need to check for duplicates and do a tiny bit of cleaning (mergeing and keeping the reqd columns)
sus_closed_hosp = sus_closed_hosp.drop_duplicates(subset=['PRVDR_NUM'])
sus_closed_hosp = sus_closed_hosp.merge(year_closure, on="PRVDR_NUM", how="left")
sus_closed_hosp = sus_closed_hosp[['FAC_NAME', 'ZIP_CD', 'YEAR_CLOSURE']]

print(len(sus_closed_hosp))
#We see 174 hospitals meet the criteria 
```

2. 
```{python}
#Sorting by hospital name and printing the first 10 rows
sus_closed_hosp = sus_closed_hosp.sort_values(by='FAC_NAME')
result = sus_closed_hosp[['FAC_NAME', 'YEAR_CLOSURE']].head(10)
print(result)
```

3. 
```{python}
#First, we will need to consolidate the dataset to find hospitals that were active in 2016. From this, we will be able to group by zip code to find the number of hospitals in each zip code

active_hosp = combined_hosp[combined_hosp ['PGM_TRMNTN_CD'] ==0]
#Briefly converting zip code to string, this will help our merge in 3.2
active_hosp.loc[:, 'ZIP_CD'] = active_hosp['ZIP_CD'].astype(str).str.split('.').str[0]
grouped_hosp = active_hosp.groupby(['ZIP_CD', 'data_year']).size().reset_index(name='NUM_HOSP')
grouped_hosp['Diff_Count'] = grouped_hosp.groupby('ZIP_CD')['NUM_HOSP'].diff()
#Converting zip code back to numeric for checking in the function
grouped_hosp['ZIP_CD'] = grouped_hosp['ZIP_CD'].astype(int)


#Next we can create a function that will gauge increases and decreases

def checker(row):
    zip = row['ZIP_CD']
    closure_year = row['YEAR_CLOSURE']
    prev_year = grouped_hosp[(grouped_hosp['ZIP_CD']== zip) & (grouped_hosp['data_year']== closure_year)]
    after_year = grouped_hosp[(grouped_hosp['ZIP_CD']== zip) & (grouped_hosp['data_year']== closure_year + 1)]

    if not prev_year.empty and not after_year.empty:
        return after_year['NUM_HOSP'].values[0] < prev_year['NUM_HOSP'].values[0]
    return True

#Now filter
sus_closed_hosp_filt = sus_closed_hosp[sus_closed_hosp.apply(checker, axis=1)]
```

a. 
```{python}
closed_hosp_names_filt = sus_closed_hosp_filt ['FAC_NAME']
merg_acq = sus_closed_hosp[~sus_closed_hosp['FAC_NAME'].isin(closed_hosp_names_filt)]
print(len(merg_acq))
#27 hospitals are suspected to be mergers and acquisitions
```

b. 
```{python}
#In order to correct for mergers and acquisitions, we chose to delete the mergers from the old dataset
merg_acq_names = merg_acq['FAC_NAME']
sus_closed_hosp_filt = sus_closed_hosp_filt[~sus_closed_hosp_filt['FAC_NAME'].isin(merg_acq_names)]

print(len(sus_closed_hosp_filt))
#147 hospitals remain after correcting for mergers
```

c.
```{python}
#Renaming the filtered dataset for clarity 
filtered_data = sus_closed_hosp_filt

#Now we print the top 10 rows of this corrected list
filtered_data = filtered_data.sort_values(by='FAC_NAME')
result2 = filtered_data [['FAC_NAME']].head(10)
print(result2)
```

2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are: xml (this stores metadata about the shapefile and information about variables), dbf (this contains attribute information in a table), prj (this contains projection and coordinate system information), shp (this is the main file that has the feature geometry), and shx(this is the index file of the feature geometry).
    b. The datasets have the following sizes: xml (16kb), dbf (6.4mb), prj (4kb), shp (837.5mb), shx (266kb)
2. 


## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
#Calculate centroids
zips_all_centroids = pset4_geo.copy()
zips_all_centroids['centroid'] = zips_all_centroids.centroid
zips_all_centroids.set_geometry('centroid')

#Information on dimensions and columns
dimensions = zips_all_centroids.shape
print("Dimensions:", dimensions)
print(zips_all_centroids.info())
#The dataframe has 33,120 observations or unique zip codes. It has 7 columns (4 object, 1 integer, and 2 geometry). The centroid column is calculated from the geometry column and is the mean of the points in the polygon. GeoID is the unique geographical identifier, ZCTAS and NAME describe the zip code, and LSAD is the legal statistical area definition. Finally, census area is the calculated area derived from the ungeneralized area.
```

2. 
```{python}
#Creating the Texas centroids GeoDataframe and sorting to check all the zip codes are in there 
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799)]
zips_texas_centroids = zips_texas_centroids.sort_values(by='ZCTA5', ascending=True)

#Creating the Texas + bordering states (New Mexico, Oklahoma, Arkansas, Louisiana) GeoDataframe and checking all codes are in there.

zips_texas_borderstates_centroids = zips_all_centroids[
    (
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(700, 714) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(716, 729) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(730, 749) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(870, 885)
    )
]
zips_texas_borderstates_centroids = zips_texas_borderstates_centroids.sort_values(by='ZCTA5', ascending=True)

#There are 1935 unique zip codes in the Texas subset, and 4057 unique codes in the Texas and its bordering states subset. 
#Source: ChatGPT to determine how to combine multiple subsets
```


3. 
```{python}
#We need to create another combination dataset for zip codes in NM, OK, AR, and LA.
border_hosp = combined_hosp[
    (combined_hosp['ZIP_CD'].astype(str).str[:3].between('750', '799') |
        combined_hosp['ZIP_CD'].str[:3].between('700', '714') |
        combined_hosp['ZIP_CD'].str[:3].between('716', '729') |
        combined_hosp['ZIP_CD'].str[:3].between('730', '749') |
        combined_hosp['ZIP_CD'].str[:3].between('870', '885')
    ) & (combined_hosp['data_year'] == 2016)
]
#Now, we group by zip code, and clean the zip codes
border_hosp_group = border_hosp.groupby('ZIP_CD').size().reset_index(name='Num_Hosp')
border_hosp_group['ZIP_CD'] = border_hosp_group['ZIP_CD'].astype(int)
border_hosp_group.rename(columns={'ZIP_CD': 'ZCTA5'}, inplace=True)
zips_texas_borderstates_centroids['ZCTA5'] = zips_texas_borderstates_centroids['ZCTA5'].astype(int)

#Merging and fixing for zip codes without hospitals (by filling with 0)
zips_withhospital_centroids = pd.merge(zips_texas_borderstates_centroids, border_hosp_group, on='ZCTA5', how='left').fillna({'Num_Hosp': 0})
#Now that we have the number of hospitals per zip code, we should check how zip codes have zero hospitals
num_zero_hospitals = (zips_withhospital_centroids['Num_Hosp'] == 0).sum()
print(num_zero_hospitals)
#As the pset question says "create a subset of zips_texas_borderstates_centroids that contains only the zip codes with at least 1 hospital in 2016", we should delete the observations that have zero hospitals
zips_withhospital_centroids = zips_withhospital_centroids[zips_withhospital_centroids['Num_Hosp'] != 0]

#We decided to merge on the zip code var - "ZCTA5". We conducted a left merge on the zips_borderstates_centroid as we wanted to maintain all the rows from the GeoDataFrame before merging in the relevant ones from our hospital data.
```

4. 
a. 
This is a computationally-intensive join. Before attempting to do the entire join,
subset to 10 zip codes in zips_texas_centroids and try the join. How long did
it take? Approximately how long do you estimate the entire procedure will take?
```{python}
#In order to do the subsetted join with 10 zip codes, lets first sort by zip code then take the top 10
zips_texas_centroids.sort_values(by='ZCTA5', inplace=True)
zips_top10_texas_centroids = zips_texas_centroids[:10]

#Now 
from timeit import default_timer as timer
start = timer()
closest_hosp = gpd.sjoin_nearest(
    zips_top10_texas_centroids,
    zips_withhospital_centroids,
    how='inner', distance_col="distance"
)
end = timer()
time = (end - start)

print(f'Time in seconds for subsetted merge: {time:.2f}')
mins = (time*(zips_texas_centroids.shape[0]/10))/60
print (f'The whole procedure will take {mins:.2f} minutes')

```


2. 
```{python}
#CRM is a way of converting something flat into spherical. Prj is projection, will tell you . it's in degres. 

```


3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 


```{python}
#Loading in data.
import geopandas as gpd 
import pandas as pd
import matplotlib.pyplot as plt
filepath = "/Users/mithilaiyer/Documents/GitHub/psetdata/shp/gz_2010_us_860_00_500k.shp" 
pset4_geo = gpd.read_file(filepath)
#Restricted to Texas zipcodes
subset_tx = pset4_geo[pset4_geo['ZCTA5'].str[:3].between('750', '799')]

#According to the question, we should be using all hospitals in 2016 in Texas. This is how we will filter - though first we should convert the zip code var to string

combined_hosp['ZIP_CD'] = combined_hosp['ZIP_CD'].astype(str).str.split('.').str[0]
tx_hosp = combined_hosp[
    (combined_hosp['ZIP_CD'].astype(str).str[:3].between('750', '799')) & 
    (combined_hosp['data_year'] == 2016)
]
#Grouping by zip code 
tx_hosp_group = tx_hosp.groupby('ZIP_CD').size().reset_index(name='Num_Hosp')

#Pre-merge cleaning. As we are merging on zip code, we will need to convert back to int and rename the hosp data zip code variable to match the spatial data
tx_hosp_group['ZIP_CD'] = tx_hosp_group['ZIP_CD'].astype(int)
tx_hosp_group.rename(columns={'ZIP_CD': 'ZCTA5'}, inplace=True)
subset_tx['ZCTA5'] = subset_tx['ZCTA5'].astype(int)

#Merging and fixing for zip codes without hospitals (by filling with 0)
merged_tex = subset_tx.merge(tx_hosp_group, on='ZCTA5', how='left')
merged_tex['Num_Hosp'].fillna(0, inplace=True)

#Chloropleth
merged_tex.plot(column='Num_Hosp', legend=True,
                cmap='coolwarm',
                legend_kwds={'label': "# Hospitals",
                    'orientation':"horizontal"},
                vmin=0, vmax=5)
plt.title('Texas Hospitals per ZIP in 2016')
plt.axis('off')
plt.show()

```
