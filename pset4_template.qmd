---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sumner Perera (CNet ID: 12403312)
    - Partner 2 (name and cnet ID): Mithila Iyer (CNet ID: 12414493)
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\S\P\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

Provider type code 01 (PRVDR_CTGRY_CD) and subtype code 01 (PRVDR_CTGRY_SBTYP_CD)
CMS Certification Number: PRVDR_NUM
Closed if its Termination Code in the POS file lists them as an “Active Provider” in 2016
- PGM_TRMNTN_CD and the value for "Active Provider" is 00
Zip code: ZIP_CD
Name of provider: FAC_NAME 
Year of suspected closure: FY_END_MO_DAY_CD 

Link to filtered view: https://data.cms.gov/provider-characteristics/hospitals-and-other-facilities/provider-of-services-file-hospital-non-hospital-facilities/data/q4-2016?query=%7B%22filters%22%3A%7B%22rootConjunction%22%3A%7B%22label%22%3A%22And%22%2C%22value%22%3A%22AND%22%7D%2C%22list%22%3A%5B%5D%7D%2C%22keywords%22%3A%22%22%2C%22offset%22%3A0%2C%22limit%22%3A10%2C%22sort%22%3A%7B%22sortBy%22%3Anull%2C%22sortOrder%22%3Anull%7D%2C%22columns%22%3A%5B%22PRVDR_CTGRY_SBTYP_CD%22%2C%22PRVDR_CTGRY_CD%22%2C%22FAC_NAME%22%2C%22PRVDR_NUM%22%2C%22PGM_TRMNTN_CD%22%2C%22ZIP_CD%22%2C%22FY_END_MO_DAY_CD%22%5D%7D 


1. After reviewing the entirety of the pset and the questions that are asked, I pulled the following variables: 

- PRVDR_CTGRY_CD: Provider type 
- PRVDR_CTGRY_SBTYP_CD: Subtype code 
- PRVDR_NUM: CMS Certification Number
- PGM_TRMNTN_CD: Provider termination code 
- ZIP_CD: Zip code 
- FAC_NAME: Provider name 
- TRMNTN_EXPRTN_DT: Date the provider was terminated 

2. 
```{python}
# Import the file from my local drive 
import pandas as pd 
import os 
base_path = r"C:\Users\12019\\Downloads"
path = os.path.join(base_path, "pos2016.csv")
pos2016 = pd.read_csv(path)

# Subset to PRVDR_CTGRY_CD = 01 and PRVDR_CTGRY_SBTYP_CD = 01 
subset_16 = pos2016[(pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1) & (pos2016["PRVDR_CTGRY_CD"] == 1)]

# Count the number of rows 
total_16 = len(subset_16)
total_16
```

Subsetting to short-term hospitals gives a dataset with 7245 total facilities. According to the linked paper, there are nearly 5,000 short-term acute care hospitals which is a smaller subset than what I found with subsetting. Another data source (https://www.aha.org/statistics/fast-facts-us-hospitals) cites a total of 6,120 hospitals in the US. Both sources cite a lower number than what we found which suggests that this number may be inaccurate. It could differ because there are hospitals listed which have actually closed or not operational. 
    
3.
```{python}
# Load in datasets 
path1 = os.path.join(base_path, "pos2017.csv")
pos2017 = pd.read_csv(path1)

path2 = os.path.join(base_path, "pos2018.csv")
pos2018 = pd.read_csv(path2, encoding='latin1')

# Note: Had to do the encoding='latin1' addition to account for errors which said: 
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x98 in position 11970: invalid start byte

path3 = os.path.join(base_path, "pos2019.csv")
pos2019 = pd.read_csv(path3, encoding='latin1')

# Drawn from ChatGPT. Asked "How do you append multiple data sets with the same columns?"
combined = pd.concat([pos2016, pos2017, pos2018, pos2019], axis=0)

# Plot the number of observations by year  
import altair as alt
alt.Chart(combined).mark_bar().encode(
  alt.X('year:Q'), 
  y='count()'
)

```



4. 
    a.
    b.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
#Drawing in combined file and checking it 
import pandas as pd
import os 
# path = "/Users/mithilaiyer/Documents/GitHub/psetdata/combined_part1.csv"
path4 = os.path.join(base_path, "combined_part1.csv")
combined_hosp = pd.read_csv(path4, low_memory=False) 
combined_hosp.head()

#Finding hospitals that are active in 2016 (where PGM_TRMNTN_CD is zero) 
active_in_2016 = combined_hosp[(combined_hosp['data_year'] == 2016) & (combined_hosp['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM'].unique()

#Finding hospitals that are closed (according to termination code)
hosp_closed = combined_hosp[(combined_hosp['PRVDR_NUM'].isin(active_in_2016)) & (combined_hosp['data_year'].isin([2017, 2018, 2019])) & (combined_hosp['PGM_TRMNTN_CD']!=0)]

#Now, we should check if there are any hospitals that may be in the data only in 2016. We can do this by subtracting the number of hospitals present after 2016 from the active hospitals in 2016
active_after_2016 = combined_hosp[combined_hosp['data_year'].isin([2017, 2018, 2019])]['PRVDR_NUM'].unique()
maybe_closure = [x for x in active_in_2016 if x not in active_after_2016]
print(maybe_closure)
#This prints an empty set, which means that there are no hospitals in the dataset operating solely in 2016

#Now we need to find the suspected year of closure. 
year_closure = hosp_closed.groupby('PRVDR_NUM')['data_year'].min().reset_index()
year_closure.columns = ['PRVDR_NUM', 'YEAR_CLOSURE']
sus_closed_hosp = combined_hosp[combined_hosp['PRVDR_NUM'].isin(year_closure['PRVDR_NUM'])]

#Now we need to check for duplicates and do a tiny bit of cleaning (mergeing and keeping the reqd columns)
sus_closed_hosp = sus_closed_hosp.drop_duplicates(subset=['PRVDR_NUM'])
sus_closed_hosp = sus_closed_hosp.merge(year_closure, on="PRVDR_NUM", how="left")
sus_closed_hosp = sus_closed_hosp[['FAC_NAME', 'ZIP_CD', 'YEAR_CLOSURE']]

print(len(sus_closed_hosp))
#We see 174 hospitals meet the criteria 
```

2. 
```{python}
#Sorting by hospital name and printing the first 10 rows
sus_closed_hosp = sus_closed_hosp.sort_values(by='FAC_NAME')
result = sus_closed_hosp[['FAC_NAME', 'YEAR_CLOSURE']].head(10)
print(result)
```

3. 
```{python}
#First, we will need to consolidate the dataset to find hospitals that were active in 2016. From this, we will be able to group by zip code to find the number of hospitals in each zip code

active_hosp = combined_hosp[combined_hosp ['PGM_TRMNTN_CD'] ==0]
grouped_hosp = active_hosp.groupby(['ZIP_CD', 'data_year']).size().reset_index(name='NUM_HOSP')
grouped_hosp['Diff_Count'] = grouped_hosp.groupby('ZIP_CD')['NUM_HOSP'].diff()

#Next we can create a function that will gaugle increases and decreases

def checker(row):
    zip = row['ZIP_CD']
    closure_year = row['YEAR_CLOSURE']
    prev_year = grouped_hosp[(grouped_hosp['ZIP_CD']== zip) & (grouped_hosp['data_year']== closure_year)]
    after_year = grouped_hosp[(grouped_hosp['ZIP_CD']== zip) & (grouped_hosp['data_year']== closure_year + 1)]

    if not prev_year.empty and not after_year.empty:
        return after_year['NUM_HOSP'].values[0] < prev_year['NUM_HOSP'].values[0]
    return True

#Now filter
sus_closed_hosp_filt = sus_closed_hosp[sus_closed_hosp.apply(checker, axis=1)]
```

a. 
```{python}
closed_hosp_names_filt = sus_closed_hosp_filt ['FAC_NAME']
merg_acq = sus_closed_hosp[~sus_closed_hosp['FAC_NAME'].isin(closed_hosp_names_filt)]
print(len(merg_acq))
#27 hospitals are suspected to be mergers and acquisitions
```

b. 
```{python}
#In order to correct for mergers and acquisitions, we delete the mergers from the old dataset
merg_acq_names = merg_acq['FAC_NAME']
sus_closed_hosp_filt = sus_closed_hosp_filt[~sus_closed_hosp_filt['FAC_NAME'].isin(merg_acq_names)]

print(len(sus_closed_hosp_filt))
#147 hospitals remain after correcting
```

c.
```{python}
#Lets rename the filtered dataset for better clarity 
filtered_data = sus_closed_hosp_filt

#Now we print the top 10 rows of this corrected list
filtered_data = filtered_data.sort_values(by='FAC_NAME')
result2 = filtered_data [['FAC_NAME']].head(10)
print(result2)
```


## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types are: xml (this stores metadata about the shapefile and information about variables), dbf (this contains attribute information in a table), prj (this contains projection and coordinate system information), shp (this is the main file that has the feature geometry), and shx(this is the index file of the feature geometry).
    b. The datasets have the following sizes: xml (16kb), dbf (6.4mb), prj (4kb), shp (837.5mb), shx (266kb)
2. 

```{python}
#Loading in data.
import geopandas as gpd 
import pandas as pd
filepath = r"C:\Users\12019\Downloads\shp\gz_2010_us_860_00_500k.shp"
pset4_geo = gpd.read_file(filepath)
pset4_geo['ZCTA5'] = pd.to_numeric(pset4_geo['ZCTA5'], errors='coerce')
#Restricted to Texas zipcodes
subset_geo = pset4_geo[(pset4_geo['ZCTA5'] >= 75000) & (pset4_geo['ZCTA5'] <= 79999)]

```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
```{python}
#Calculate centroids
zips_all_centroids = pset4_geo.copy()
zips_all_centroids['centroid'] = zips_all_centroids.centroid
zips_all_centroids.set_geometry('centroid')

#Information on dimensions and columns
dimensions = zips_all_centroids.shape
print("Dimensions:", dimensions)
print(zips_all_centroids.info())
#The dataframe has 33,120 observations or unique zip codes. It has 7 columns (4 object, 1 integer, and 2 geometry). The centroid column is calculated from the geometry columm and is the mean of the points in the polygon. GeoID is the unique geographical identifier, ZCTAS and NAME describe the zip code, and LSAD is the legal statistical area definition. Finally, census area is the calculated area derived from the ungeneralized area.
```

2. 
```{python}
#Creating the Texas centroids GeoDataframe and sorting to check all the zip codes are in there 
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799)]
zips_texas_centroids = zips_texas_centroids.sort_values(by='ZCTA5', ascending=True)

#Creating the Texas + bordering states (New Mexico, Oklahoma, Arkansas, Louisiana) GeoDataframe and checking all codes are in there.

zips_texas_borderstates_centroids = zips_all_centroids[
    (
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(750, 799) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(700, 714) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(716, 729) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(730, 749) |
        zips_all_centroids['ZCTA5'].str[:3].astype(int).between(870, 885)
    )
]
zips_texas_borderstates_centroids = zips_texas_borderstates_centroids.sort_values(by='ZCTA5', ascending=True)

#There are 1935 unique zip codes in the Texas subset, and 4057 unique codes in the Texas and its bordering states subset. 
#Source: ChatGPT to determine how to combine multiple subsets
```

2. 
```{python}
#CRM is a way of converting something flat into spherical. Prj is projection, will tell you . it's in degres. 

```


3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
```{python}
#Using the clean data, create a subset with only TX zip codes
zip_codes_tx = filtered_data[(filtered_data['ZIP_CD'] >= 75000) & (filtered_data['ZIP_CD'] <= 79900)]

#Drawn from ChatGPT. Asked "Let's say I have a data frame with a column of values. How do I create a new table that shows me in one column all the unique values in that column and how many times they appeared?" This gives a table with all Texas zip codes and the number of closures within each zip code. 
tx_zips_closures = zip_codes_tx["ZIP_CD"].value_counts().reset_index()

```

2. 
```{python}
#Plot the cloropleth only for the zip codes that we identified in the previous question. Drawn from ChatGPT. Asked "If I have a list of values in a dataframe that are a subset of values from a column in a geopandas shp file. How do I plot only those subset in a cloropleth?"

#First, create a list of the unique values and filter the geo shp data to only those zips in TX
import matplotlib.pyplot as plt 
tx_zips_closures["ZCTA5_Value"] = tx_zips_closures["ZIP_CD"]/100
subset_values = tx_zips_closures["ZCTA5_Value"].astype(int)
subset2_geo = subset_geo[subset_geo["ZCTA5"].isin(subset_values)]

#Next, plot the cloropleth
subset2_geo.plot(column ='ZCTA5', color= "brown")

#Figure out the number of these regions. This gives 27 zip codes in Texas that have at least 1 closure in 2016-2019. 
print(len(tx_zips_closures))
```

3. 

```{python}
#Already created a GeoDataFrame of the directly affected zip codes called subset2_geo. Now, create a 10 mile buffer around them. Gained help from ChatGPT, asked "How to create a X mile buffer around a geodataframe?"
subset2_geo = subset2_geo.to_crs(epsg=32633)
buffer_m = 10 * 1609.34
subset2_geo.geometry.buffer(buffer_m).plot(edgecolor="black")
plt.axis("off")
plt.show()

#Gathered information from ChatGPT, asked "How to do a spatial join between two geodata frames?"

#Start with creating a geodataframe for the buffered regions 
tx_zips_buffered = subset2_geo.copy()
tx_zips_buffered["geometry"] = tx_zips_buffered.geometry.buffer(buffer_m)
tx_zips_buffered = gpd.GeoDataFrame(geometry=tx_zips_buffered['geometry'], crs=subset2_geo.crs)

#Next, with a bit of renaming first, do the join to the geodataframe only for TX
all_tx = subset_geo.to_crs(epsg=32633)
joined = gpd.sjoin(tx_zips_buffered, all_tx, how="inner", predicate="intersects")

#How many indirectly affected zip codes are there?
print(len(joined))
```

There are 24 indirectly affected zip codes after completing the spatial join. 

4. 

```{python}

```

## Reflecting on the exercise (10 pts) 

1. 
